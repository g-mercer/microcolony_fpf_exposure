---
title: "degradation-model"
author: "Guy Mercer"
date: "09/08/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

```{r}
library("mkin", quietly = TRUE)
library(tidyverse)

# import data
nectar <- read.csv(file = "./input/nectar_residues_ugkg.csv")

pollen <- read.csv(file = "./input/pollen_residues_ugkg.csv")

# keep only data post second application (t=0)
nectar_day_1 <- nectar [nectar$DALA_h > 0,] 
pollen_day_1 <- pollen [pollen$DALA_h > 0,] 

# change colnames
colnames(nectar_day_1) <- c("t", "parent")
colnames(pollen_day_1) <- c("t", "parent")
```

The three models shown are SFO, FOMC and DFOP corresponding to simple first order (SFO), the case of declining rate constant over time (FOMC) and the case of two different phases of the kinetics (DFOP).

```{r}
# put input in correct format
nectar_model_data <- mkin_wide_to_long(nectar_day_1)
pollen_model_data <- mkin_wide_to_long(pollen_day_1)

# generate nectar models and plot
nectar_models <- mmkin(c("SFO", "FOMC", "DFOP"), cores = 1,
               list("nectar" = nectar_model_data), quiet = TRUE)

plot(nectar_models)

nectar_models_SFO <- mmkin("SFO", cores = 1,
               list("nectar" = nectar_model_data), quiet = TRUE)


plot(nectar_models_SFO)

summary(nectar_models [["SFO", 1]])

# generate pollen models and plot
pollen_models <- mmkin(c("SFO", "FOMC", "DFOP"), cores = 1,
               list("pollen" = pollen_model_data), quiet = TRUE)

plot(pollen_models)

# summary(pollen_models [["SFO", 1]])
```

Clearly none of the models fit nectar very well. For pollen can't even fit a model. Try with only datapoints past 24h. 

```{r}
test <- mkinfit("FOMC", nectar_model_data, quiet = TRUE)

summary(test)

plot(test, show_errmin = TRUE, show_residuals = TRUE, main = "test")

test1 <- mkinfit("DFOP", nectar_model_data, quiet = TRUE)

summary(test1)

plot(test1, show_errmin = TRUE, show_residuals = TRUE, main = "test")
```


```{r}
# for nectar 

# filter out data between 1 and 24 and bind
nectar_day_24 <- nectar_day_1 [nectar_day_1$t >= 24,] 

# put input in correct format
nectar_model_data_24 <- mkin_wide_to_long(nectar_day_24)

# generate nectar models and plot
nectar_models_24 <- mmkin(c("SFO", "FOMC", "DFOP"), cores = 1,
               list("nectar" = nectar_model_data_24), quiet = TRUE)

plot(nectar_models_24)

nectar_models_24_SFO <- mmkin("SFO", cores = 1,
               list("nectar" = nectar_model_data_24), quiet = TRUE)

plot(nectar_models_24_SFO)

summary(nectar_models_24 [["SFO", 1]])
```

```{r}
# for pollen

# filter out data between 1 and 24 and bind
pollen_day_24 <- pollen_day_1 [pollen_day_1$t >= 24,] 

# put input in correct format
pollen_model_data_24 <- mkin_wide_to_long(pollen_day_24)

# generate pollen models and plot
pollen_models_24 <- mmkin(c("SFO", "FOMC", "DFOP"), cores = 1,
               list("pollen" = pollen_model_data_24), quiet = TRUE)

plot(pollen_models_24)

# below was to check if the output was the same for pollen_model_24 on the ggplot markdown. It is. It seems that there is an override function
# built into mkin where if a model completely fails the output is a horizontal line even if the model parameters actually result in a slight curve.
pollen_model_24 <- mkinfit("SFO", pollen_model_data_24, quiet = TRUE)

plot(pollen_model_24)

parms(pollen_model_24) 

# summary(pollen_models_24 [["DFOP", 1]])
```

No improvement for pollen but for nectar the SFO model warrants further investigation (22.1% χ2 error level). Systematic error in the residual plot, with overestimation for low values and underestimation for high values. Also, strong correlation between parent_0 and log_k_parent. Not suitable. For an explanation of chi-square test page 86 ([FOCUS](https://esdac.jrc.ec.europa.eu/public_path/projects_data/focus/dk/docs/finalreportFOCDegKinetics.pdf)), then appendix 3 (page 282) for examples of it being used to assess model fit. Field data example 1 (Page 295) is a good example. The error to pass the χ2 test (χ2 error level) at alpha = 0.05 can be a pretty high percentage (22%) and the model still be suitable. Looking at the actual graph and the residual plots seems a better way for model selection. The systematic error in my models is the main issue that precludes their use. 

For comparing between models selecting the model with the lowest χ2 error value is easy to interpret. 

Regarding the χ2 test FOCUS writes, 

**The χ2 significance test indicates whether the model is probably not appropriate, i.e. demonstrating that the differences between calculated and observed are unlikely due to chance. Often α = 0.05 is used, that is a χ2 greater than χ2m,0.05 indicates that the probability that the model is not appropriate is greater than 95 %. If χ2 > tabulated χ2m,α, then the model is not appropriate at the chosen level of significance. The χ2 test considers deviations between observed and predicted values relative to the uncertainty of the measurements. Usually this isn't known. Therefore, a percent error value is scaled with the mean of all the observed values. For this reason the χ2 test uses observed mean values.The minimum error (error-% / 100 * mean observed) at which the test is passed can be calculated. The test is passed if the calculated value of χ2 is equal to or smaller than the standard tabulated value at the 5% significance level and the given degrees of freedom. The model with the smallest error percentage is defined as most appropriate, because it describes the measured data in the most robust way. Field data will be inherently more variable than laboratory data generated under controlled conditions. Therefore, for field studies, the error percentages at which χ2 passes will generally be larger than for laboratory studies**

A less esoteric definition of χ2 error level is, *the experimental error that has to be assumed in order to explain the data from the model*. Thus, a model provides a better description of the data if less experimental error has to be assumed in order to explain the data.

My interpretation of the above is there is no cutoff for the minimum error % value to pass the test (χ2 error level) because it is situation specific. Indeed, FOCUS write, *"Note that there is no inherent value for the percent error for any given test system. The selection of an acceptable value is purely pragmatic"*. For example, for field data the χ2 error level will generally be larger due to its inherent variability relative to lab studies. Use the χ2 error level for comparison between models and look at observed vs predicted/calculated and residuals vs time graphs for assessing model fit of individual models. There needs to be no systematic error present in the residuals. In other words, residuals need to appear random.

Refs:

[FOCUS Report](https://esdac.jrc.ec.europa.eu/public_path/projects_data/focus/dk/docs/finalreportFOCDegKinetics.pdf)

[mkin Examples](https://pkgdown.jrwb.de/mkin/articles/FOCUS_L.html#laboratory-data-l3-1)


